After building each "main" or "basic" functionality as per the requirements in the doc,
I would test to see if the output was the expected output. This document details how I built each function
and how they were tested as a result (usually just.. print statements. lots. of print statements.)

DATA_HANDLING.PY:

Testing process_multiple_videos(video_folder), def extract_audio(video_path, output_dir), 
def transcribe_audio(audio_path):

Testing for this was straightforward. At this point I hadn't implemented the multiple file processing functionality
so I directly passed the path of the video file over to extract_audio(). I checked the audio_file that appeared
in my directory to see if it matched the .mp4 file. And I printed the transcription generated by whisper,
and played the original video file to see if the dialogue matched.

Testing store_segments(transcript):

Thankfully, whisper already provides a transcript with text and timestamp keys. And it naturally segments 
the transcript based on pauses. I leveraged that to generate
a transcript_dict. I printed the transcript_dict to ensure that all timestamps were less than 5 seconds. There is a pytest called tests.py
in the testing folder on the github repo that tests to see if all timestamps are less than 5 seconds long, as per 
the document requirement.

Testing segment_audio(audio_path, output_dir):

I know the instructions state to segment the audio file first, but in my case it made much more sense
to take the already naturally segmented whisper file and ensure each segment was less than 5 seconds.
I took the timestamps generated by store_segments, creating new .wav files from the original audio files
based on those timestamps. I then checked a few of the segmented audio files in the newly generated folder, 
audio_chunks, to see if they matched the timestamps on the segmented transcription.

Testing analyze_segment_sentiment(transcript_dict):

Used hugging face model to predict the sentiment for each phrase. Checked via.. print statement to ensure that 
all the phrases had their sentiments analysed, and the predictions weren't wildly incorrect.
The initial model that I used (distilbert-base-multilingual-cased-sentiments-student) was different from the one currently in use 
since I realised that some of its sentiment predictions were rather silly. 

Testing convert_dataframe(sentiment_results, video_name, first_sentiment_results=None):

Reformatted the sentiment_results dict so that it was in the specified csv format the requirements doc wanted.
Saved csv file. Opened csv file to check for proper format. Changed up some lines of the codes if not. Ran the code again.
Checked again.

DATA_MANIPULATION.PY

