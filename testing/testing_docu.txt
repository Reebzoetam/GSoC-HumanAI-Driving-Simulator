After building each "main" or "basic" functionality as per the requirements in the doc,
I would test to see if the output was the expected output. This document details how I built each function
and how they were tested as a result (usually just.. print statements. lots. of print statements.)

DATA_HANDLING.PY:

Testing process_multiple_videos(video_folder), def extract_audio(video_path, output_dir), 
def transcribe_audio(audio_path):

Testing for this was straightforward. At this point I hadn't implemented the multiple file processing functionality
so I directly passed the path of the video file over to extract_audio(). I checked the audio_file that appeared
in my directory to see if it matched the .mp4 file. And I printed the transcription generated by whisper,
and played the original video file to see if the dialogue matched.

Testing store_segments(transcript):

Thankfully, whisper already provides a transcript with text and timestamp keys. And it naturally segments 
the transcript based on pauses. I leveraged that to generate
a transcript_dict. I printed the transcript_dict to ensure that all timestamps were less than 5 seconds. There is a pytest called tests.py
in the testing folder on the github repo that tests to see if all timestamps are less than 5 seconds long, as per 
the document requirement.

Testing segment_audio(audio_path, output_dir):

I know the instructions state to segment the audio file first, but in my case it made much more sense
to take the already naturally segmented whisper file and ensure each segment was less than 5 seconds.
I took the timestamps generated by store_segments, creating new .wav files from the original audio files
based on those timestamps. I then checked a few of the segmented audio files in the newly generated folder, 
audio_chunks, to see if they matched the timestamps on the segmented transcription.

Testing analyze_segment_sentiment(transcript_dict):

Used hugging face model to predict the sentiment for each phrase. Checked via.. print statement to ensure that 
all the phrases had their sentiments analysed, and the predictions weren't wildly incorrect.
The initial model that I used (distilbert-base-multilingual-cased-sentiments-student) was different from the one currently in use 
since I realised that some of its sentiment predictions were rather silly. 

Testing convert_dataframe(sentiment_results, video_name, first_sentiment_results=None):

Reformatted the sentiment_results dict so that it was in the specified csv format the requirements doc wanted.
Saved csv file. Opened csv file to check for proper format. Changed up some lines of the codes if not. Ran the code again.
Checked again.

DATA_MANIPULATION.PY

Testing create_time_buckets(transcript_path):

oh.. this is a big one. First began by taking the latest timestamp and incrementing by 5 seconds to create 
entries in the time_buckets list for each time bucket increment. Then parsed through the csv file generated
by the previous script line by line to see if the timestamp fell within the time bucket time.
To test, I once again.. printed time_buckets to see if the correct timestamp in the mm:ss format corresponded
to the correct phrases.

I also wanted to count the number of positive, neutral, negative sentiment occurences for each time bucket. 
As I parsed through the original csv file, I counted the sentiment results and averaged the confidence scores
corresponding to each sentiment. To test, I printed data, and checked the lengths of each key to ensure that
no values were omitted. I also indexed a few entries just to check if it corresponded with the original csv file.
There's always a good chance I shifted data entries around by an index or two.

The overall code is quite ugly and could honestly be much more elegant (all those for loops and if statements..)
but it was most readable for me that way so I could debug things. I hope to write more elegant and efficient code
in the future.

Testing plot_histogram(csv_filename):

Displayed plot. Did it show correct values in the desired way knowing what data we have? 
Did it display it in a readable manner? If yes, move on. If not, tweak the code until it does.

Testing plot_sentiment(csv_filename):

Slightly more complicated than plot_histogram. I first plotted the stacked bars by colour based on sentiment counts 
for each time bucket. Once that display was satisfactory, I moved onto an additional feature I wanted to add.

I also wanted to add a gradient based on what the average confidence
score was for each positive, neutral, negative sentiment result was. So I had to normalize the confidence scores first
and use cmap to adjust the intensity of the colour of each stacked bar based on the confidence score. 
If there were multiple sentiments of the same sentiment value, I averaged them.
I also displayed the gradients to the right of the graph as a legend for users to understand how high 
each confidence score was for each sentiment.
